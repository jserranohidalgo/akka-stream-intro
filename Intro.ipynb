{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`com.typesafe.akka::akka-stream:2.6.4`\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.time._\n",
    "import scala.concurrent._, duration._\n",
    "import akka._\n",
    "import akka.actor._\n",
    "import akka.stream._\n",
    "import akka.stream.scaladsl._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A streaming DSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Akka streams](https://doc.akka.io/api/akka/current/akka/stream/index.html) offers a DSL for programming reactive stream processors. These programs are made up from three basic components: sources, flows and sinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Source`s represent data publishers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val source: Source[Int, NotUsed] = Source(List(1,2,3,4,5,6,7,8,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sink`s are data consumers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sink: Sink[Any, Future[Done]]  = \n",
    "    Sink.foreach(((a: Any) => println(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect a source and sink in order to obtain a so-called _runnable graph_, i.e. a streaming processor that can be actually run. In DSL terminology, the `RunnableGraph` is the _program_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph1: RunnableGraph[NotUsed] = source.to(sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run a graph we need a _materializer_, i.e. the interpreter of the streaming program. The standard materializer offered by akka-stream builts upon actors, so we need an actor system first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit lazy val system: ActorSystem = ActorSystem(\"akka-stream-primer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to explicitly instantiate the materializer, since it's already available implicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicitly[Materializer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the sources and sinkes we can attach _flows_, intermediate processing steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph2 = source.via(Flow[Int].map((i: Int) => i + 1)).to(sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can log the activity of each operator in a graph to properly understand the contribution of each step in the transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import akka.event.Logging\n",
    "\n",
    "implicit class SourceOps[A, M](S: Source[A, M]){\n",
    "    def logAll(l: String): Source[A, M] =\n",
    "    S.log(l).withAttributes(Attributes.logLevels(\n",
    "        onElement = Logging.WarningLevel,\n",
    "        onFinish = Logging.WarningLevel,\n",
    "        onFailure = Logging.DebugLevel))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source(List(1,2,3)).logAll(\"source\")\n",
    "    .via(Flow[Int].map(_ + 1)).logAll(\"flow\")\n",
    "    .to(Sink.ignore)\n",
    "    .run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materialized values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a pipeline is called the *materialized value*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val source = Source(List(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ignoreS = Sink.ignore\n",
    "val toListS = Sink.collection[Int,List[Int]]\n",
    "val toPrintlnS = Sink.foreach(println)\n",
    "val foldS = Sink.fold[String, Int](\"\")((acc: String, e: Int) => acc + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.to(ignoreS)\n",
    "source.to(toListS)\n",
    "source.to(foldS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.toMat(ignoreS)((mvl: NotUsed, mvr: Future[Done]) => mvr)\n",
    "source.toMat(toListS)(Keep.left)\n",
    "source.toMat(foldS)((mvl: NotUsed, mvr: Future[String]) => mvr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common shortcuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.toMat(foldS)(Keep.right).run\n",
    "source.runWith(foldS)\n",
    "source.runFold(\"\")(_+_)\n",
    "\n",
    "source.to(Sink.foreach(println))\n",
    "source.runForeach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async boundaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akka streams vs. iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the difference between the previous akka stream program and the following `Iterator` program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).iterator.map(_ + 1).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, we obtain a streaming processor. However, in akka streams, intermediate processing steps and actions performed over the resulting data, are first-class entities: flows and sinks. They can be defined independently, reused and combined as we wish. There is no such notion in the iterator realm. Moreover, akka streams are compiled into actors, and the graph has the potential to run asyncronously and concurrently, with back-pressure niceties. Iterator programs are run sequentially and syncronously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting parallelism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit val ec: ExecutionContext = system.dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source(1 to 3)\n",
    "    .mapAsync(1) { i: Int =>\n",
    "        println(s\"A: $i\"); Future(i)\n",
    "    }\n",
    "    .mapAsync(1) { i: Int =>\n",
    "        println(s\"B: $i\"); Future(i)\n",
    "    }\n",
    "    .map { i =>\n",
    "        println(s\"C: $i\"); Future(i)\n",
    "    }\n",
    "    .to(Sink.ignore)\n",
    "    .run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can create several substreams in parallel as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source(1 to 9)\n",
    "    .flatMapMerge(3, a => {\n",
    "        println(a); \n",
    "        Source(List(-a)).map{b => println(b); b}\n",
    "    }).runWith(Sink.ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source(1 to 9)\n",
    "    .flatMapConcat(a => {\n",
    "        println(a); \n",
    "        Source(List(-a)).map{b => println(b); b}\n",
    "    }).runWith(Sink.ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fan-in, fan-out & additional operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val source1: Source[Int, NotUsed] = Source(List(1,2,3,4))\n",
    "val source2: Source[String, NotUsed] = Source(List(\"hola\", \"adios\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1.map(i => s\"num: $i\")\n",
    "    .merge(source2)\n",
    "    .runForeach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1.zip(source2)\n",
    "    .runWith(Sink.foreach(println))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1.map(_.toString)\n",
    "    .concat(source2)\n",
    "    .runWith(Sink.foreach(println))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://doc.akka.io/docs/akka/current/stream/stream-substream.html, for an explanation of substreams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file._, akka.util._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = Paths.get(\"Intro.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileIO.fromPath(file)\n",
    "Framing.delimiter(\n",
    "    ByteString(\"\\n\"), \n",
    "    maximumFrameLength = 1500, \n",
    "    allowTruncation = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines: Source[String, Future[IOResult]] = \n",
    "    FileIO.fromPath(file)\n",
    "        .via(Framing.delimiter(ByteString(\"\\n\"), maximumFrameLength = 1500, allowTruncation = true))\n",
    "        .throttle(1, 10.millisecond)\n",
    "        .map(_.utf8String)\n",
    "    //    .runWith(Sink.takeLast(3))\n",
    "    //    .runWith(Sink.foreach(println))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.runForeach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileIO.toPath(Paths.get(\"linecounts.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.map(_.length)\n",
    "//    .map(_.toString+\"\\n\")\n",
    "    .map(i => ByteString(i.toString + \"\\n\"))\n",
    "    .runWith(FileIO.toPath(Paths.get(\"linecounts.txt\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
